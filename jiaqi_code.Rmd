---
title: "Jiaqi"
author: "Li Sun"
date: "October 12, 2015"
output: html_document
---

# Load packages
```{r}
suppressWarnings(library(dplyr))
suppressWarnings(library(ggplot2))
suppressWarnings(library(gplots))
suppressWarnings(library(RColorBrewer))
suppressWarnings(library(lsr))
suppressWarnings(library(ICC))
suppressWarnings(library(lme4))
suppressWarnings(library(car))
```


# Load data
```{r}
setwd("C:/Users/user/Dropbox/jiaqi")
setwd("/home/li/Dropbox/jiaqi")
raw <- read.csv("Dissertation_Survey.csv", stringsAsFactors=FALSE)
#Select only needed variables
df <- raw[, c(18,19, 12,13,14,15,16,17,20, 43:46,47, 65:74)]
names(df) <- c("if_counseled", "whynot", "gender", "age", "degree", "stay","marital","religion","english", "sla1", "sla2", "sla3", "sla4", "sla_id", "y1", "y2", "y3", "y4", "y5", "y6", "y7", "y8", "y9", "y10")
```

# Data cleaning

1. variables needed to be edited: 'age', 'stay', 'english'
There is one value which is "adult", we change that one to 18, according to the info that he has 89 TOFLE score and stayed in US for 18 month. We change this to 25, which is average age of F1 student who came here after undergraduate (23.5) and master study(26.5). There is also another invalid entry which is 3. We believe it is imput error. and change it to 30
For stay, there is an entry which is "whole life", we change that to 12 * age.

```{r}
################### age
# There is one value which is "adult", we change that one to 18, according to the info that he has 89 TOFLE score and stayed in US for 18 month. We change this to 25, which is average age of F1 student who came here after undergraduate (23.5) and master study(26.5). There is also another invalid entry which is 3. We believe it is imput error. and change it to 30
df$age[df$age=="adult"] <- "25"
df$age[df$age==" "] <- NA
df$age <- as.numeric(df$age)

################### stay
wlidx <- df$stay=="Whole Life"
df$stay[df$stay == " "] <- NA
df$stay<- gsub("months?", "", df$stay)
df$stay[(!grepl("[0-9]", df$stay))&!is.na(df$stay)] <- "0"
for(i in seq(df$stay)){
  if(grepl(" *([0-9]+) ?ye?a?rs?.*", df$stay[i])){
    df$stay[i] <-gsub(" *([0-9]+) ?ye?a?rs?.*", "\\1", df$stay[i])
    df$stay[i] <-as.character(as.numeric(df$stay[i])*12)
  }
}
df$stay <- as.numeric(gsub(" ", "", df$stay))
# The value "whole life" is change to the person's age times 12
df$stay[wlidx] <- df$age[wlidx] * 12

################## english
df$english
eng <- df$english
eng <- gsub("\\D*([0-9.-]*)\\D*", "\\1", eng)
data.frame(ori=df$english, new=eng)
# convert the IELTS score to TOEFL score according to 'https://www.ets.org/toefl/institutions/scores/compare/'
eng[grep("6.5", eng)] <- "79"
# Convert all non-sense value (ex. "65-120", "0", "") to NA
eng[eng=="0"|eng=="65-120"|eng==""] <- NA
# remove any characters after "-" in order to convert this to numeric
eng <- gsub("-.*", "", eng)
eng <- as.numeric(eng)
hist(eng, col="blue", breaks=100, ylab = "counts", xlab="TOEFL Score", main = "English Proficiency")

# We do see three different types of test been reported here. So lets convert them to one scale based on IBT

# Convert PBT score to IBT score
eng2<-numeric()
for(i in seq(eng)){
  if(is.na(eng[i])){eng2[i]<-NA;next}
  x<-eng[i]
  if(x<=120){
    if(x >= 111){eng2[i] <- "111-120"}
    else if(x>=96){eng2[i] <- "96-110"}
    else if(x>=79){eng2[i] <- "79-95"}
    else if(x>=65){eng2[i] <- "65-78"}
    else if(x < 65){eng2[i] <- "0-65"}
  }
  else if(eng[i] <=450){
    if(x >= 273){eng2[i] <- "111-120"}
    else if(x>=243){eng2[i] <- "96-110"}
    else if(x>=213){eng2[i] <- "79-95"}
    else if(x>=183){eng2[i] <- "65-78"}
    else if(x < 183){eng2[i] <- "0-65"}
  }
  else{
    if(x >= 640){eng2[i] <- "111-120"}
    else if(x>=590){eng2[i] <- "96-110"}
    else if(x>=550){eng2[i] <- "79-95"}
    else if(x>=513){eng2[i] <- "65-78"}
    else if(x < 513){eng2[i] <- "0-65"}
  }
}
data.frame(ori=df$english, new2=eng2)
table(eng2)
df$english <- eng2
```

2. Construct new variable "SL-ASIA values score" ("SLval"), based on the SUINN-LEW ASIA SCALE (question 22~23)
coding:
1. Do not believe ...... 5. strongly believe

```{r}
df$SLval <- rep(0, nrow(df))
for(i in 1:nrow(df)){
  if(is.na(df[i,10])|is.na(df[i,11])){df$SLval[i] <- NA;next}
  if(df[i,10] <= 2){
    if(df[i,11] <= 2){df$SLval[i] <- "N"}
    else if(df[i, 11] ==3){df$SLval[i] <- "(W)"}
    else if(df[i, 11] > 3){df$SLval[i] <- "W"}
  }
  else if(df[i,10] == 3){
    if(df[i,11] <= 2){df$SLval[i] <- "(A)"}
    else if(df[i, 11] ==3){df$SLval[i] <- "(B)"}
    else if(df[i, 11] > 3){df$SLval[i] <- "W"}
  }
  else{
    if(df[i,11] <= 3){df$SLval[i] <- "A"}
    else{df$SLval[i] <- "B"}
  }
}
### If you only want 4 groups with no "()"
df$SLval4 <- gsub("[()]", "", df$SLval)
table(df$SLval4)
```

Construct new variable "SL-ASIA behavioral competencies score" ("SLcom"), based on the SUINN-LEW ASIA SCALE (question 24~25)
1. do not fit ......  5. fit very well

```{r}
df$SLcom <- rep(0, nrow(df))
for(i in 1:nrow(df)){
  if(is.na(df[i,12])|is.na(df[i,13])){df$SLcom[i] <- NA;next}
  if(df[i,12] <= 2){
    if(df[i,13] <= 2){df$SLcom[i] <- "N"}
    else if(df[i, 13] ==3){df$SLcom[i] <- "(W)"}
    else if(df[i, 13] > 3){df$SLcom[i] <- "W"}
  }
  else if(df[i,12] == 3){
    if(df[i,13] <= 2){df$SLcom[i] <- "(A)"}
    else if(df[i, 13] ==3){df$SLcom[i] <- "(B)"}
    else if(df[i, 13] > 3){df$SLcom[i] <- "W"}
  }
  else{
    if(df[i,13] <= 3){df$SLcom[i] <- "A"}
    else{df$SLcom[i] <- "B"}
  }
}
### If you only want 4 groups with no "()"
df$SLcom4 <- gsub("[()]", "", df$SLcom)
table(df$SLcom4)
table(df[,c(26,28)])
```

Construct new variable "SL-ASIA self-identity score" ("sla_id"), based on the SUINN-LEW ASIA SCALE (question 24~25)
coding
26
1. I consider myself basically an Asian person (e.g., Chinese, Japanese, Korean, Vietnamese, etc.). Even though I live and work in America, I still view myself basically as an Asian person.
5. I consider myself basically as an American. Even though I have an Asian background and characteristics, I still view myself basically as an American.
7. I consider myself as an Asian-American, although deep down I always know I am an Asian.
9. I consider myself as an Asian-American, although deep down, I view myself as an American first.
10. I consider myself as an Asian-American. I have both Asian and American characteristics, and I view myself as a blend of both.

```{r}
class(df$sla_id)
table(df$sla_id)
```

We found `r sum(df$sla_id==1, na.rm=T)/sum(!is.na(df$sla_id))` responders chose 1 and there is one value "2" which is not included in coding rubric and we will change it to 9 because 9 is missing here.
```{r}

class(df$sla_id)
df$sla_id[df$sla_id==2&!is.na(df$sla_id)] <- 9
# re-order the factors
df$sla_id <- factor(as.factor(df$sla_id), levels=c("1", "7", "10", "9", "5"))
```

Construct new val indicating individual attitude to counseling.
for all original question
values:
1. strongly disgree
2. disgree
3. agree
4. strongly agree
Calculating based on
Whittlesey, V. (2001). Diversity activities for psychology. Boston: Allyn and
Bacon, and Fischer, E., and Farina, A. (1995). Attitudes toward seeking psychological
professional help: A shortened form and considerations for research. Journal of College Student
Development, 36, 368-373.

```{r}
#Reverse several questions' scores
df$y2 <- 5-df$y2
df$y4 <- 5-df$y4
df$y8 <- 5-df$y8
df$y9 <- 5-df$y9
df$y10 <- 5-df$y10
df$y <- rowSums(df[,15:24])
hist(df$y, breaks=15, col="light blue", xlab ="scores of attitude seeking professional help")

```

# Exploratory data analysis
Visualize the relationship among the 3 scores from "Suinn-Lew Asian Self Identity Acculturation"
1. Visualize the relationship between different grouping methods of individuals acculturation
```{r}
### Based on 7 groups
x <- data.frame(val=factor(df$SLval, levels=c("A", "(A)", "N", "B", "(B)", "(W)", "W")), com=factor(df$SLcom, levels=c("A", "(A)", "N", "B", "(B)", "(W)", "W")), id = df$sla_id)
x2<-x[complete.cases(x),]
ggplot(data = x2, aes(x = val, y = com, colour=id)) + stat_sum(aes(size = factor(..n..)), geom = "point") + scale_size_discrete(range = c(3, 20)) + labs(title="3 ACCULTURATION SCORES based on 7 groupss", x="SL-ASIA values score", y="SL-ASIA behavioral competencies score", colour = "SL-ASIA self-identity score", size="number of individual")                                                 
### Based on 4 groups
x <- data.frame(val=factor(df$SLval4, levels=c("A", "N", "B", "W")), com=factor(df$SLcom4, levels=c("A", "N", "B", "W")), id = df$sla_id)
x2<-x[complete.cases(x),]
ggplot(data = x2, aes(x = val, y = com, colour=id)) + stat_sum(aes(size = factor(..n..)), geom = "point") + scale_size_discrete(range = c(3, 20))+ labs(title="3 ACCULTURATION SCORES based on 7 groupss", x="SL-ASIA values score", y="SL-ASIA behavioral competencies score", colour = "SL-ASIA self-identity score", size="number of individual")   
```

From this two figures, we found asian students tend to purposely maintain their asian value even they have sufficient competence to get into western culture. We also found that the most asian students identify themselves as asian no matter how good they fit in western life.

2. Visualize the relationship between the 3 scores and attitudes for seeking professional counseling.
This need 
```{r}
x <- data.frame(val=factor(df$SLval4, levels=c("A", "N", "B", "W")), com=factor(df$SLcom4, levels=c("A", "N", "B", "W")), id = df$sla_id, y=df$y)
x2<-x[complete.cases(x),]
# The relationship between our acculturation scores and our response
ggplot(aes(x=val, y=y, fill=val), data=x2) + geom_boxplot(outlier.colour = "red", outlier.size = 5)
ggplot(aes(x=com, y=y, fill=com), data=x2) + geom_boxplot(outlier.colour = "red", outlier.size = 5)
ggplot(aes(x=as.numeric(id), y=y, colour=id), data=x2) + stat_sum(aes(size = factor(..n..)), geom="point", alpha=0.8) + scale_size_discrete(range = c(5,20))

# Overall plot
forplotly <- x2
forplotly <- summarise(group_by(x2, val, com, id), number = n(), y = mean(y))
## 
```
3D plot here:
https://plot.ly/~rikku1983/35/visualization-of-acculturation-and-attitude-for-seeking-professional-counseling/?share_key=0poL7ODE8G2eg9itKxkbs6

Now lets start making data ready for modeling
1. Missing values
```{r, echo=FALSE}
## Get rid of unnecessary columns
df2 <- df[,c(29, 3:8, 26,28, 14)]
dfwEng <- df[,c(29, 3:9, 26,28, 14)]
sum(complete.cases(df2))
## Delete all rows with all NAs
df3 <- df2[rowSums(!is.na(df2))!=0,]
missing_palette <- colorRampPalette(c("gray", "black"))(n = 1000)
heatmap.2(matrix(as.numeric(is.na(df3)), ncol=10), Rowv=NA, Colv=NA, scale = "none", na.rm=F, col=missing_palette, dendrogram="none", trace="none", key=F, margins = c(3, 12))
table(rowSums(is.na(df3)))
## According to this table, 89 complete cases and 22 cases with only one missing value, most likely the english. So our strategy is to keep rows with at most 1 missing values
df4 <- df3[rowSums(is.na(df3))<=1,]
heatmap.2(matrix(as.numeric(is.na(df4)), ncol=10), Rowv=NA, Colv=NA, scale = "none", na.rm=F, col=missing_palette, dendrogram="none", trace="none", key=F, margins = c(3, 12))
df4<-df4[complete.cases(df4),]
```
Now we have 110 rows and 10 cols with 2 NAs, both in our response of interest. So for our following analysis, we need to use only 110 full cases from this data. 

2. impute nonsense values
```{r}
apply(df4, 2, range)
df4$age[df4$age==3]<-30
```
There is one individual with age 3, According to all information from this row. we decide to impute this one to 30. 

3. Convert all variable type to ones ready for analysis
```{r}
sapply(df4, class)
#gender
df4$gender[df4$gender==1] <- "man"
df4$gender[df4$gender==2] <- "woman"
df4$gender <- factor(df4$gender, levels=c("man", "woman"))
#degree
df4$degree[df4$degree == 1] <- "undergraduate"
df4$degree[df4$degree == 2] <- "master"
df4$degree[df4$degree == 3] <- "doctoral"
df4$degree <- factor(df4$degree, levels = c("undergraduate", "master", "doctoral"))
#marital
df4$marital[df4$marital==1] <- "single"
df4$marital[df4$marital==2] <- "married"
df4$marital <- factor(df4$marital, levels=c("single", "married"))
#religion
df4$religion[df4$religion==1] <- "christian"
df4$religion[df4$religion==2] <- "atheist"
df4$religion[df4$religion==3] <- "Buddhist"
df4$religion <- factor(df4$religion, levels=c("atheist", "christian", "Buddhist"))
#english
#df4$english <- factor(df4$english, levels = c("0-65", "65-78", "79-95", "96-110", "111-120"))
#SLval4
df4$SLval4 <- factor(df4$SLval4, levels = c("A","N","B","W"))
#SLcom4
df4$SLcom4 <- factor(df4$SLcom4, levels = c("A","N","B","W"))
#sla_id is used as continuous numeric variable
df4$sla_id <- as.numeric(df4$sla_id)
```

# Analysis of relationship between each variables
In this part, we start to look into relationship between different variables in this table by studying there correlations
```{r}
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 1/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * r+1)
}
pairs(df4, panel=panel.smooth, diag.panel = panel.hist, lower.panel = panel.cor)
par(mfrow=c(2,2))
hist(df4$age)
hist(df4$stay)
hist(log(df4$age))
hist(df4$stay^(1/3))

df5 <- mutate(df4, stay=stay^(1/3), age=log(age), sla_id =as.numeric(sla_id))
pairs(df5, panel=panel.smooth, diag.panel = panel.hist, lower.panel = panel.cor)
```
Compare df4 and df5, the correlation between age and response drop from 0.0075 to 0.0042, and the correlation between stay and response increase from 0.083 to 0.089. Basically, not very significant change were observed. So we will live with non-transformed data.
Among our predictors, we observe highest correlations between age and marital status (0.55) and, age and degree(0.46). 

Now lets build the effect size matrix.
for numeric vs numeric: Pearson's correlation is used, absolute value of this r is categorized as followed, Effect size  r
Small	      0.10
Medium	    0.30
Large	      0.50[1][2]

for numeric vs categorical: R square from one-way ANOVA is used


for categorical vs categorical: Cramer's V
df*  small	medium	large
1	     .10	   .30	  .50
2	     .07	   .21	  .35
3	     .06	   .17	  .29

```{r}
#summary(aov(df4$y~df4$gender));
corr<-df4[1:10,];corr[,]<-0
corr <- as.matrix(corr)
diag(corr) <- 1
rownames(corr) <- colnames(corr)
N <- nrow(df4)
for(i in 1:(ncol(df4)-1)){
  for(j in (i+1):ncol(df4)){
  type <- (class(df4[,i])=="numeric") + (class(df4[,j])=="numeric")
  print(type)
  if(type==0){
    corr[i,j] <- cramersV(df4[,i], df4[,j])
  }
  else if(type==1){
    if(class(df4[,i])=="numeric"){
      sumsq <- summary(aov(df4[,i]~df4[,j]))[[1]][,2]
      corr[i,j] <- sumsq[1]/(sumsq[1]+sumsq[2])
    }
    else{
      sumsq <- summary(aov(df4[,j]~df4[,i]))[[1]][,2]
      corr[i,j] <- sumsq[1]/(sumsq[1]+sumsq[2])
    }
  }
  else{
    corr[i,j] <- cor(df4[,i], df4[,j])
  }
  }
}

# My function of calculating ICC by using random effect model on one response ~ one grouping variable
# myicc <- function(formu, dat){
#   model <- lmer(formula = formu, data=dat)
#   variance <- as.data.frame(VarCorr(model))$vcov
#   icc <- variance[1]/(variance[1] + variance[2])
#   return(icc)
# }

##y~age
# mgen <- lmer(y~1+(1|gender), data=df4)
# vargen <- as.data.frame(VarCorr(mgen))$vcov
# iccgen <- vargen[1]/(vargen[1]+vargen[2])
# 
# myicc(y~1+(1|gender), df4)
# 
# lm(y~gender, data=df4)
# icc1 <- 0.4509^2/(0.4509^2+2.637^2)
# ICCest(gender, y, df4)$ICC
#agrees

##y~degree
# mdeg<-lmer(y~1+(1|degree), data=df4)
# vardeg <- as.data.frame(VarCorr(mdeg))$vcov
# iccdeg <- vardeg[1]/(vardeg[1]+vardeg[2])
# 
# summary(lm(y~degree, data=df4))
# ICCest(degree, y, df4)$ICC
```
##visualization
```{r}
corr[lower.tri(corr)] <- NA
rownames(corr) <- colnames(corr)
corr_palette <- colorRampPalette(c("red", "black","green"))(n = 1000)
heatmap.2(corr, Rowv=NA, Colv=NA, scale = "none", na.rm=F, col=corr_palette, dendrogram="none", trace="none", key=F, margins = c(3, 12))
```

##multicolinearity
```{r}
# df6 <- df4[,-1]
# multicolin <- numeric()
# for(i in 1:9){
#   df7 <- df6[,-i]
# }

```


# Fit regression
1. Model without interaction

```{r, echo=FALSE, results='hide', message=FALSE}
fm <- lm(y~., df4)
summary(fm)
fmstep <- step(fm)
summary(fmstep)
```

2. Model with interactions

```{r, echo=FALSE, results='hide', message=FALSE}
fmint <- lm(y~.*., df4)
summary(fmint)
fmintstep <- step(fmint)
summary(fmintstep)
```

Model with all possible interactions after backward selection, has much higher R squared but also much more predictors, and the design matrix is not full rank any more. So colinearity and multicolinearity is brought in. 
Let's try less interaction. We will try just one variable interacting with all others to see if there would be any improvement of r squared.


```{r, echo=FALSE, results='hide', message=FALSE}
genderm <- lm(y~gender*., df4)
agem <- lm(y~age*., df4)
degreem <- lm(y~degree*., df4)
staym <- lm(y~stay*., df4)
maritalm <- lm(y~marital*., df4)
religionm <- lm(y~religion*., df4)
slval4m <- lm(y~SLval4*., df4)
slcom4m <- lm(y~SLcom4*., df4)
sla_idm <- lm(y~sla_id*., df4)
single_var_int<-data.frame(full_without_int=summary(fm)$adj.r.squared, gender=summary(genderm)$adj.r.squared, age=summary(agem)$adj.r.squared, degree=summary(degreem)$adj.r.squared, stay=summary(staym)$adj.r.squared, marital=summary(maritalm)$adj.r.squared, religion=summary(religionm)$adj.r.squared, SLval4=summary(slval4m)$adj.r.squared, SLcom4=summary(slcom4m)$adj.r.squared, sla_id=summary(sla_idm)$adj.r.squared)
rownames(single_var_int) <- "adj_r_squared"
```

```{r, echo=FALSE, message=FALSE}
single_var_int
```

We found, when we include interactions of the following 4 variables to all others, we got dramatically boosted the adjusted r squared.
degree
stay
SLval4
sla_id

```{r, echo=FALSE, results='hide', message=FALSE}
fmint2 <- lm(y~stay*. + degree*. + SLval4*. + sla_id*., df4)
summary(fmint2)
fmint2step <- step(fmint2)
summary(fmint2step)
# SLval4 interaction introduce alias columns
fmint3 <- lm(y~stay*. + degree*. + sla_id*., df4)
summary(fmint3)
fmint3step <- step(fmint3)
summary(fmint3step)

#SLcom4 is introducing alias columns and itself is not almost independent of our y
df6 <- df4[,-9]
fmint4 <- lm(y~stay*. + degree*. + sla_id*., df6)
summary(fmint4)
fmint4step <- step(fmint4)
summary(fmint4step)
```

The following model is from data without SLcom4
First we try all combinations of the 4 variables interacting with all others from the 4 listed above which gave us best improved adjusted r squared. And all full models are backward selected.

```{r, echo=FALSE, results='hide', message=FALSE}
##
mstay <- lm(y~. + stay:., df6)
summary(mstay)
mstaystep <- step(mstay)
summary(mstay)

mdg <- lm(y~. + degree:., df6)
summary(mdg)
mdgstep <- step(mdg)
summary(mdgstep)

mid <- lm(y~. + sla_id:., df6)
summary(mid)
midstep <- step(mid)
summary(midstep)

mval <- lm(y~. + SLval4:., df6)
summary(mval)
mvalstep <- step(mval)
summary(mvalstep)

m_dg <- lm(y~. + stay:. + sla_id:. + SLval4:., df6)
summary(m_dg)
m_dgstep <- step(m_dg)
summary(m_dgstep)

m_stay <- lm(y~. + degree:. + sla_id:. + SLval4:., df6)
summary(m_stay)
m_staystep <- step(m_stay)
summary(m_staystep)

m_id <- lm(y~. + degree:. + stay:. + SLval4:., df6)
summary(m_id)
m_idstep <- step(m_id)
summary(m_idstep)

m_val <- lm(y~. + degree:. + stay:. + sla_id:., df6)
summary(m_val)
m_valstep <- step(m_val)
summary(m_valstep)

## with 2 
m1 <- lm(y~. + degree:. + stay:., df6)
summary(m1)
m1step <- step(m1)
summary(m1step)

m2 <- lm(y~. + degree:. + sla_id:., df6)
summary(m2)
m2step <- step(m2)
summary(m2step)

m3 <- lm(y~. + degree:. + SLval4:., df6)
summary(m3)
m3step <- step(m3)
summary(m3step)

m4 <- lm(y~. + SLval4:. + stay:., df6)
summary(m4)
m4step <- step(m4)
summary(m4step)

m5 <- lm(y~. + sla_id:. + stay:., df6)
summary(m5)
m5step <- step(m5)
summary(m5step)

m6 <- lm(y~. + sla_id:. + SLval4:., df6)
summary(m6)
m6step <- step(m6)
summary(m6step)
```

All models we have 
fmstep: full model without interactions
    all following are with interactions
fmintstep: with all possible interaction
fmintstep2: with four variables interacting with all others: degree, stay, SLval4, sla_id
fmintstep3: with three variables interacting with all others: degree, stay, sla_id
fmintstep4: with three variables interacting with all others: degree, stay, sla_id, and without variable SLcom4
mstaystep: with stay interacting with all others
mdgstep: with degree interacting with all others
midstep: with sla_id interacting with all others
mvalstep: with SLval4 interacting with all others
m_staystep: without stay interacting with all others
m_dgstep: without degree interacting with all others
m_idstep: without sla_id interacting with all others
m_valstep: without SLval4 interacting with all others
m1step: with degree and stay interacting with all others
m2step: with degree and sla_id interacting with all others
m3step: with degree and SLval4 interacting with all others
m4step: with stay and SLval4 interacting with all others
m5step: with stay and sla_id interacting with all others
m6step: with SLval4 and sla_id interacting with all others
sl1step: with sla_id interacting with degree, s
              tay interacting with SLval4
              degree interacting with age
              SLval4 interacting with marital
              stay interacting with sla_id
              age interacting with stay
              age interacting with sla_id
sl2step: forward select from sl1step
sl3: remove alias variable from sl2step
sl4: remove triple interacting variable

## Compare all models
```{r, echo=FALSE, results='hide', message=FALSE}
aic <- AIC(fmstep, fmintstep, fmint2step, fmint3step, fmint4step, mstaystep, mdgstep, midstep, mvalstep, m_staystep, m_dgstep, m_idstep, m_valstep, m1step, m2step, m3step, m4step, m5step, m6step)
bic <- BIC(fmstep, fmintstep, fmint2step, fmint3step, fmint4step, mstaystep, mdgstep, midstep, mvalstep, m_staystep, m_dgstep, m_idstep, m_valstep, m1step, m2step, m3step, m4step, m5step, m6step)
compare <- cbind(aic, bic$BIC)
compare$adj.rsqr <- rep(0, nrow(bic))
compare$adj.rsqr[1] <- summary(fmstep)$adj.r.squared
compare$adj.rsqr[2] <- summary(fmintstep)$adj.r.squared
compare$adj.rsqr[3] <- summary(fmint2step)$adj.r.squared
compare$adj.rsqr[4] <- summary(fmint3step)$adj.r.squared
compare$adj.rsqr[5] <- summary(fmint4step)$adj.r.squared
compare$adj.rsqr[6] <- summary(mstaystep)$adj.r.squared
compare$adj.rsqr[7] <- summary(mdgstep)$adj.r.squared
compare$adj.rsqr[8] <- summary(midstep)$adj.r.squared
compare$adj.rsqr[9] <- summary(mvalstep)$adj.r.squared
compare$adj.rsqr[10] <- summary(m_staystep)$adj.r.squared
compare$adj.rsqr[11] <- summary(m_dgstep)$adj.r.squared
compare$adj.rsqr[12] <- summary(m_idstep)$adj.r.squared
compare$adj.rsqr[13] <- summary(m_valstep)$adj.r.squared
compare$adj.rsqr[14] <- summary(m1step)$adj.r.squared
compare$adj.rsqr[15] <- summary(m2step)$adj.r.squared
compare$adj.rsqr[16] <- summary(m3step)$adj.r.squared
compare$adj.rsqr[17] <- summary(m4step)$adj.r.squared
compare$adj.rsqr[18] <- summary(m5step)$adj.r.squared
compare$adj.rsqr[19] <- summary(m6step)$adj.r.squared
```

```{r, echo=FALSE, message=FALSE}
compare
```


So far we have tried different variables interacting with all others, which might bring many irrelevant interactions to compromise our model in BIC values. So what about smaller number of interactions?
According to all models above, I picked following several interactions
sla_id and degree
stay and SLval4
degree and age
SLval4 and marital
stay and sla_id
age and stay
age and sla_id
All of them are maintained in many of above models after backward selection.

```{r, echo=FALSE, results='hide', message=FALSE}
sl1 <- lm(y~. + sla_id:degree + stay:SLval4 + degree:age + SLval4:marital + stay:sla_id + age:stay + age:sla_id, df4)
summary(sl1)
sl1step<-step(sl1)
summary(sl1step)

compare <- rbind(compare, c(21,AIC(sl1step), BIC(sl1step), summary(sl1step)$adj.r.squared))
rownames(compare)[20] <- "sl1step"
```

This new model is better than ones with similar number of variables. Try forward selection to see if we can get any more significant interactions
```{r, echo=FALSE, results='hide', message=FALSE}
sl2step <- step(sl1step, lm(y~.*., df6)$call, direction="forward")
summary(sl2step)
compare <- rbind(compare, c(33,AIC(sl2step), BIC(sl2step), summary(sl2step)$adj.r.squared))
rownames(compare)[21] <- "sl2step"
```
sl2step looks the best so far, we will adopt this model and further improve it.
there is a question with this model that the design matrix is not full rank. The variable degreeDoc interacting marital status is linear combination of other variables. So lets remove one variable to make it full rank.

```{r, echo=FALSE, message=FALSE}
alias(sl2step)
```
so basically the original variable marital is the same as degreeDocmarital + degreeMasmarital, lets remove marital variable
```{r, echo=FALSE, results='hide', message=FALSE}
sl3 <- lm(y ~ age + degree + stay + SLval4 + sla_id + degree:sla_id + stay:SLval4 + age:degree + stay:sla_id + age:sla_id + gender:SLval4 + age:marital + degree:marital + marital:SLval4 + degree:stay + age:degree:sla_id, df4)
summary(sl3)
compare <- rbind(compare, c(33,AIC(sl3), BIC(sl3), summary(sl3)$adj.r.squared))
rownames(compare)[22] <- "sl3"
```
remove 3 variable interaction
```{r, echo=FALSE, results='hide', message=FALSE}
sl4 <- lm(y ~ age + degree + stay + SLval4 + sla_id + degree:sla_id + stay:SLval4 + age:degree + stay:sla_id + age:sla_id + gender:SLval4 + age:marital + degree:marital + marital:SLval4 + degree:stay, df4)
summary(sl4)
compare <- rbind(compare, c(31 ,AIC(sl4), BIC(sl4), summary(sl4)$adj.r.squared))
rownames(compare)[23] <- "sl4"

sl5 <- lm(y~degree + stay + marital + SLval4 + sla_id + SLcom4 + age:sla_id + sla_id:degree + age:degree + stay:SLval4, df4)
summary(sl5)
sl5step <- step(sl5)
summary(sl5step)
compare <- rbind(compare, c(18 ,AIC(sl5step), BIC(sl5step), summary(sl5step)$adj.r.squared))
rownames(compare)[24] <- "sl5step"

```
Compare all models, model sl3 stands out
```{r, echo=FALSE}
kable(compare)
```
The best model is based on the criterion above is sl3.


more
```{r}
dfwEng$gender[dfwEng$gender==1] <- "man"
dfwEng$gender[dfwEng$gender==2] <- "woman"
dfwEng$gender <- factor(dfwEng$gender, levels=c("man", "woman"))
#degree
dfwEng$degree[dfwEng$degree == 1] <- "undergraduate"
dfwEng$degree[dfwEng$degree == 2] <- "master"
dfwEng$degree[dfwEng$degree == 3] <- "doctoral"
dfwEng$degree <- factor(dfwEng$degree, levels = c("undergraduate", "master", "doctoral"))
#marital
dfwEng$marital[dfwEng$marital==1] <- "single"
dfwEng$marital[dfwEng$marital==2] <- "married"
dfwEng$marital <- factor(dfwEng$marital, levels=c("single", "married"))
#religion
dfwEng$religion[dfwEng$religion==1] <- "christian"
dfwEng$religion[dfwEng$religion==2] <- "atheist"
dfwEng$religion[dfwEng$religion==3] <- "Buddhist"
dfwEng$religion <- factor(dfwEng$religion, levels=c("atheist", "christian", "Buddhist"))
#english
dfwEng$english <- factor(dfwEng$english, levels = c("0-65", "65-78", "79-95", "96-110", "111-120"))
#SLval4
dfwEng$SLval4 <- factor(dfwEng$SLval4, levels = c("A","N","B","W"))
#SLcom4
dfwEng$SLcom4 <- factor(dfwEng$SLcom4, levels = c("A","N","B","W"))

dfwEng <- dfwEng[complete.cases(dfwEng),]

fitwEng <- lm(y~., dfwEng)
summary(fitwEng)
fitwEng2 <- step(fitwEng)
summary(fitwEng2)
```

# More parsimoneous model
Check the best model we chose. Too many interactions are making interpretation extremely hard and multicolinearity very severe, which means the variance is dramatically inflated and the coefficient is not stable. Considering the small sample size in this case, 110. too many variables are reducing the degree of freedom which means we are probably overfitting. However, there must be interactions occuring because including interactions are boosting our adjusted r squared without increasing BIC very much. The AIC is even dropping. So we believe there are interactions but we need to pinpoint several most significant interactions to add to the model without losing interprebility.  
In order to reduce multicolinearity also, let's transform and center the numeric variable and change the levels of categorical variable to make sure the reference level is the most abundant one
```{r}
df7 <- df4
df7$gender <- relevel(df7$gender, "woman")
df7$age <- scale(log(df7$age), scale=F)
df7$degree <- relevel(df7$degree, "doctoral")
df7$stay <- scale((df7$stay)^(1/3), scale=F)
df7$SLval4 <- relevel(df7$SLval4, "B")
df7$SLcom4 <- relevel(df7$SLcom4, "B")
```
Two things have been done:
1. relevel all categorical variable to make most abundant level as reference level
2. Transformation and centering
stay is centered after cube root
age is centered after log

Now lets fit some model
```{r}
l1 <- step(lm(y~., df7))
# Because we are interested more in SLval and our response, so lets and interacting terms involving SLval4
lval <- step(lm(y~. + SLval4:stay + SLval4:religion, df7))
lcom <- step(lm(y~. + SLcom4:., df7))
lvc <- step(lm(y~. + SLval4:degree + SLcom4:degree, df7))
l2 <- step(lm(y ~ age + degree + stay + SLval4 + sla_id + degree:sla_id + stay:SLval4 + age:degree + stay:sla_id + age:sla_id + gender:SLval4 + age:marital + degree:marital + marital:SLval4 + degree:stay, df7))
summary(l2)
AIC(l2)
BIC(l2)

lfor <- step(lval, lm(y~.*., df7)$call, direction = "forward")
l3 <- step(lm(y~.+ sla_id:., df7))
alias(l3)


x1 <- step(lm(y~.+gender:.,df7))
summary(x1)
x2 <- step(lm(y~.+age:.,df7))
summary(x2)
x3 <- step(lm(y~.+degree:.,df7))
summary(x3)
x4 <- step(lm(y~.+stay:.,df7))
summary(x4)
x5 <- step(lm(y~.+marital:.,df7))
summary(x5)
x6 <- step(lm(y~.+religion:.,df7))
summary(x6)
x7 <- step(lm(y~.+SLval4:.,df7))
summary(x7)
x8 <- step(lm(y~.+SLcom4:.,df7))
summary(x8)
x9 <- step(lm(y~.+sla_id:.,df7))
summary(x9)

x10 <- lm(y~.+age:sla_id+sla_id:degree+sla_id:SLcom4+SLcom4:SLval4+age:degree+degree:stay+stay:SLval4, df7)
summary(x10)
x10step <- step(x10)
summary(x10step)

x11 <- lm(y~age + degree + stay + marital + SLval4 + sla_id + age:sla_id + sla_id:degree + sla_id:SLcom4 + age:degree + degree:stay + stay:SLval4, df7)
summary(x11)
x11step <- step(x11)
summary(x11step)

x12 <- lm(y~degree + stay + marital + SLval4 + sla_id + SLcom4 + age:sla_id + sla_id:degree + age:degree + stay:SLval4, df4)
summary(x12)
x12step <- step(x12)
summary(x12step)
```
Try leaps package
```{r}
library(leaps)
# test <- regsubsets(y~.*., df4, nvmax=5, really.big=T)

```
dosent work

Reference
1. Jacob Cohen (1988). Statistical Power Analysis for the Behavioral Sciences (second ed.). Lawrence Erlbaum Associates.
2. Cohen, J (1992). "A power primer". Psychological Bulletin 112 (1): 155-159. doi:10.1037/0033-2909.112.1.155. PMID 19565683.